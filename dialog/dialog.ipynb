{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facebook/blenderbot-400M-distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "\n",
    "bot_name = \"blenderBot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: hello, my name is Nero\n",
      "blenderBot:  Hi Nero, nice to meet you. What do you like to do in your spare time?\n",
      "human: what is my name?\n",
      "blenderBot:  Hi, Nero. I like to play video games. Do you have any hobbies?\n",
      "human: what do you do for work?\n",
      "blenderBot:  I don't work, I'm a student. What about you? Do you work or go to school?\n",
      "human: what is my name?\n",
      "blenderBot:  I am not sure what your name is, but I am a student as well. What are you studying?\n",
      "human: where do you work?\n",
      "blenderBot:  I do not have a job right now. I am still in high school. What is your major?\n",
      "human: nice\n",
      "blenderBot:  I do not have a job right now. I am still in high school. What is your major?\n"
     ]
    }
   ],
   "source": [
    "utterances = []\n",
    "# [\n",
    "#     \"hello\", \"Hello! How are you doing today? I just got back from a walk with my dog.\", \n",
    "#     \"Where did you walk?\", \"I took my dog for a walk in the park. It was nice to get some fresh air.\", \n",
    "#     \"Is the air outside in your city clean?\", \"Yes, it is very nice out today. I hope it stays that way for a while.\",\n",
    "#     \"That sounds lovely\", \"It really is. I love it when the weather is nice. What do you like to do for fun?\"\n",
    "# ]\n",
    "# \"</s> <s>\".join(utterances)\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(6):\n",
    "    utterance = input(\">> User:\")\n",
    "    # utterance = \"My friends are cool but they eat too many carbs.\"\n",
    "    utterances.append(utterance)\n",
    "\n",
    "    print(\"human: {}\".format(utterance))\n",
    "\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    inputs = tokenizer([\"</s> <s>\".join(utterances)], return_tensors='pt', truncation=True, max_length=128)\n",
    "\n",
    "    reply_ids = model.generate(**inputs)\n",
    "\n",
    "    response = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "    utterances.append(response)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"{}: {}\".format(bot_name, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo\n",
    "\n",
    "human: hello\n",
    "blenderBot:  Hello! How are you doing today? I just got back from a walk with my dog.\n",
    "human: Where did you walk?\n",
    "blenderBot:  I took my dog for a walk in the park. It was nice to get some fresh air.\n",
    "human: Is the air outside in your city clean?\n",
    "blenderBot:  Yes, it is very nice out today. I hope it stays that way for a while.\n",
    "human: That sounds lovely\n",
    "blenderBot:  It really is. I love it when the weather is nice. What do you like to do for fun?\n",
    "Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\n",
    "human: I like to code. Where did you go today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s>\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microsoft/DialoGPT-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "bot_name = \"dialoGPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blenderBot:  are you doing today? I just got back from a walk with my dog.\n",
      "blenderBot: \n",
      "blenderBot: \n",
      "blenderBot: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's chat for 5 lines\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     new_user_input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>> User:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# append the new user input tokens to the chat history\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     bot_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([chat_history_ids, new_user_input_ids], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m new_user_input_ids\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"{}: {}\".format(bot_name, tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
